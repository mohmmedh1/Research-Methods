\# Unit 5 – Interviews, Survey Methods, and Questionnaire Design



\## 1. Overview

This unit expanded on both \*\*qualitative\*\* and \*\*quantitative\*\* research tools by introducing the use of \*\*interviews\*\*, \*\*surveys\*\*, and \*\*questionnaires\*\* as core methods for collecting empirical data.  

As Saunders, Lewis and Thornhill (2023) highlight, these methods provide researchers with complementary approaches—allowing both \*depth\* (through interviews) and \*breadth\* (through surveys).  



A key insight was understanding the distinction between \*\*a survey\*\* and \*\*a questionnaire\*\*.  

While a \*survey\* encompasses the entire process of data collection and analysis, a \*questionnaire\* represents the structured instrument used within it.  

This distinction is critical in cybersecurity research, where clarity of measurement and validity of questions determine the reliability of insights drawn from user or institutional data.



---



\## 2. Application to My Research Context

In relation to my research topic—\*cloud security risks in higher education\*—the use of questionnaires and surveys will serve two main purposes:

1\. \*\*Assessing organisational awareness\*\* of cloud security risks and compliance responsibilities.  

2\. \*\*Evaluating the effectiveness\*\* of current mitigation strategies and institutional governance frameworks.  



Interviews with IT and compliance officers will provide qualitative perspectives on governance culture, while surveys distributed among academic and administrative staff will yield quantitative evidence of security-practice maturity.  

Together, these complementary data sources will support a \*\*mixed-methods approach\*\*, enhancing both contextual understanding and statistical credibility.



---



\## 3. Data Collection Planning

In preparation for data collection, I explored the use of \*\*pre-testing\*\* and \*\*post-testing\*\* as a way to measure change over time—for example, assessing staff knowledge of cybersecurity policies before and after awareness training sessions.  

This technique could be applied within my future dissertation project to evaluate the impact of risk-awareness initiatives.



The \*\*e-Portfolio reflective activity\*\* prompted me to identify which collection tools best align with my research questions:

\- \*\*Structured questionnaires\*\* to capture quantifiable data on compliance awareness.  

\- \*\*Semi-structured interviews\*\* to explore attitudes toward privacy and risk management.  

\- \*\*Online distribution methods\*\* (via institutional forms or Microsoft Forms) to ensure accessibility, efficiency, and ethical oversight.



This planning stage also reinforced the importance of defining \*\*population\*\* and \*\*sample\*\*, ensuring representativeness while maintaining confidentiality—particularly when participants are internal to an institution.



---



\## 4. Artefacts and Activities

The following artefacts document my progress and application of Unit 5 concepts:



| Artefact | Description | File |

|-----------|--------------|------|

| \*\*Questionnaires\*\* | Developed sample questions assessing awareness, compliance, and perceived risk in cloud adoption. | \[Questionnaires.md](unit04/Questionnaires.md) |

| \*\*Questionnaire Analysis Critique\*\* | Analytical review of an e-book purchasing survey, identifying flaws in question logic, sampling, and data analysis. | \[Questionnaire Analysis Critique – E-Book Purchase.md](unit04/Questionnaire%20Analysis%20Critique%20E-Book%20Purch.md) |

| \*\*Privacy Case Study\*\* | Explored data-handling ethics and policy enforcement within higher education. | \[Privacy Case Study.md](unit04/Privcy%20case%20study%20.md) |



Each activity enhanced my ability to evaluate questionnaire design quality, identify sampling biases, and connect ethical principles to digital data collection.



---



\## 5. Reflection – Ethics and Reliability in Data Collection

Through this unit, I realised that \*\*questionnaire design is both a scientific and ethical exercise\*\*.  

Poorly worded questions, ambiguous scales, or leading phrasing can distort findings and compromise validity.  

Hence, reliability depends on clarity, neutrality, and respondent comprehension.



In cybersecurity contexts—where privacy, trust, and confidentiality are paramount—ethical considerations extend beyond consent forms.  

They involve protecting respondents’ data, anonymising sensitive information, and ensuring that questions neither imply blame nor disclose organisational vulnerabilities.



This reflective process helped me connect the \*\*ethical governance frameworks\*\* (BCS, ACM, and GDPR) with practical design principles.  

It strengthened my ability to create data-collection instruments that are technically sound, ethically robust, and contextually relevant to my research.



---



\## 6. Reflection on Pre-testing and Post-testing

Unit 5 also introduced \*\*pre-testing\*\* as a validation step to evaluate questionnaire reliability before full deployment.  

Conducting a pilot survey can identify unclear terminology, inconsistent scales, or logical gaps, thereby improving data quality.  

Similarly, \*\*post-testing\*\* offers a way to measure the effect of interventions (such as awareness training) on risk perception or behavioural change.  



In my future project, I plan to integrate both validation steps to strengthen credibility and ensure that survey instruments meet academic and ethical standards.



---



\## 7. References

\- Boza, T. (2022) \*How to Write a Literature Review: Six Steps to Get You from Start to Finish.\*  

\- British Research Methodology (BRM). (n.d.) \*Research Design.\*  

\- NIST. (2018) \*Framework for Improving Critical Infrastructure Cybersecurity.\*  

\- Saunders, M., Lewis, P. and Thornhill, A. (2023) \*Research Methods for Business Students\* (9th edn). Pearson Education.  

\- Shaughnessy, J. J., Zechmeister, E. B. and Zechmeister, J. S. (2019) \*Research Methods in Psychology.\* McGraw-Hill Education.  



---



\*End of Unit 5 Evidence\*



